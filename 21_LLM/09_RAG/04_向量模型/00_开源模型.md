# 1. 向量模型
## 1.1 短向量模型

1. bge-base-zh-v1.5
    - 模型下载：https://modelscope.cn/models/AI-ModelScope/bge-base-zh-v1.5/summary
    - 代码：https://github.com/Alibaba-NLP/RankingGPT

2. bge-large-zh-v1.5
   - 模型下载：https://modelscope.cn/models/Xorbits/bge-large-zh-v1.5/summary
   - 代码：https://github.com/Alibaba-NLP/RankingGPT

## 1.2 长向量模型

1. Jina AI文本向量模型v2-base-中英双语
   - 模型下载：https://modelscope.cn/models/jinaai/jina-embeddings-v2-base-zh/summary
   - 支持中英双语的文本向量模型，它支持长达8192字符的文本编码。 该模型的研发基于BERT架构(JinaBERT)，
     JinaBERT是在BERT架构基础上的改进，首次将ALiBi应用到编码器架构中以支持更长的序列。

## 1.3 基于LLM的向量模型

1. e5-mistral-7b-instruct
   - 模型下载：https://huggingface.co/intfloat/e5-mistral-7b-instruct
   - 论文：Improving Text Embeddings with Large Language Models，https://arxiv.org/pdf/2401.00368.pdf
   - 论文：Text Embeddings by Weakly-Supervised Contrastive Pre-training
   - Github代码（非官方，26k Stars）：https://github.com/kamalkraj/e5-mistral-7b-instruct
   - （基线E5模型）官放相关代码和数据：https://github.com/microsoft/unilm/tree/master/e5

    E5-mistral-7b-instruct利用LLM产生了接近100种语言的高质量且多样化的训练数据，利用纯decoder的LLM在合成数据上进一步finetune。
    仅依靠合成数据训练得到的text embedding可以媲美目前主流的sota模型，而混合合成数据跟真实标注数据训练完成的text embedding
    模型在BEIR跟MTEB上都达到新的sota效果。

2. RankingGPT-qwen-7b
    - 模型下载：https://modelscope.cn/models/iic/RankingGPT-qwen-7b/summary
    - 代码：https://github.com/Alibaba-NLP/RankingGPT
    - text ranker based on large language models with significant in-domain and out-domain effectiveness. 
    - RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement
    - 论文：https://arxiv.org/pdf/2311.16720.pdf

    ```python
        import torch
      from modelscope import AutoTokenizer, AutoModelForCausalLM
   
      tokenizer = AutoTokenizer.from_pretrained('damo/RankingGPT-qwen-7b',trust_remote_code=True)
      model = AutoModelForCausalLM.from_pretrained('damo/RankingGPT-qwen-7b',trust_remote_code=True, device_map='cpu').eval()
   
      query='when should a baby walk'
      document='Most babies start to walk around 13 months, but your baby may start walking as early as 9 or 10 months or as late as 15 or 16 months.'
   
      context=f'Document: {document} Query:'
      example=context+query
   
      context_enc = tokenizer.encode(context, add_special_tokens=False)
      continuation_enc = tokenizer.encode(query, add_special_tokens=False)
      model_input = torch.tensor(context_enc+continuation_enc[:-1])
      continuation_len = len(continuation_enc)
      input_len, = model_input.shape
   
   
      with torch.no_grad():
          logprobs = torch.nn.functional.log_softmax(model(model_input.unsqueeze(dim=0))[0], dim=-1)[0]
   
      logprobs = logprobs[input_len-continuation_len:]
      logprobs = torch.gather(logprobs, 1, torch.tensor(continuation_enc).unsqueeze(-1)).squeeze(-1)
      score = torch.sum(logprobs)/logprobs.shape[0]
   
      print(f"Document: {document[:20] + '...'} Score: {score}") 
       ```
   
# 2. ReRanker重排模型

1. bge-reranker-large
   - 模型下载：https://modelscope.cn/models/quietnight/bge-reranker-large/summary
   - 代码：https://github.com/FlagOpen/FlagEmbedding
