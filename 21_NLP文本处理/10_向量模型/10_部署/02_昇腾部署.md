本教程未bge-reranker教程，bge-embedding参考使用

# 1. 简介

- 部署服务器：昇腾910A
- 使用推理框架：torch-npu，Flag-embedding：https://github.com/FlagOpen/FlagEmbedding 
- 模型：bge-m3-reranker-v2

- 目标：在昇腾推理
- 方法：利用torch-npu和Flag-embedding (支持torch-npu)，可直接在npu推理
- 问题：变长输入推理速度慢(特别超长序列512以上)，固定长推理速度快
- 解决方法：
  - 固定长和batch推理，长度按照需要的最长设定，不够的都补到最长 (可能导致资源浪费，实测32G只能支持到4*4096)
  - 模型需要进行预热，因为第一次会非常慢，约10分钟
  - 多卡推理暂未研究

# 2. 使用详解

注意：如果环境安装遇到问题，可以直接使用llama-factory的镜像，使用方法见：ai_wiki\31_LLM\05_训练\04_优秀项目\01_llama_factory\01_昇腾使用.md

安装torch-npu

```bash
pip install torch-npu
```

修改FlagEmbedding代码，使其支持固定长推理

https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/flag_reranker.py

compute_score函数修改

```python
 inputs = self.tokenizer(
                sentences_batch,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=max_length,
            ).to(self.device)
```

改为

```python
 inputs = self.tokenizer(
                sentences_batch,
                padding="max_length",
                truncation=True,
                return_tensors='pt',
                max_length=max_length,
            ).to(self.device)
```

比如，max_length设定为4096，这样每次都会补齐到4096

之后直接调用这个源码，或者编译后调用库包也行。

```
git clone https://github.com/FlagOpen/FlagEmbedding.git
cd FlagEmbedding
pip install -e .
```

# 参考

[1] Transformers从零到精通教程——Tokenizer，https://blog.csdn.net/qq_49821869/article/details/132174930?ydreferer=aHR0cHM6Ly9jbi5iaW5nLmNvbS8%3D