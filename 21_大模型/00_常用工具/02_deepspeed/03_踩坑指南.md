1. deepspeed配置不一致问题
    
    特别在开启zero3时，提示deepspeed参数配置和训练传入的参数不一致，需要保持一致。
    其他参数可以射程‘auto’以自动获取启动脚本中的传参。
    
   ```json
     "train_batch_size": 24,
     "train_micro_batch_size_per_gpu": 1,
     "gradient_accumulation_steps": 1,
      "bf16": {
       "enabled": false
        },
        "fp16": {
          "enabled": true,
          "loss_scale": 0,
          "loss_scale_window": 100
        },
   ```
   
   - 如果启动脚本中加入--fp16, deepspeed中需要设置true或者auto 
   - 集群启动时，特别在使用zero3时，不支持设置auto，train_batch_size = 
     train_micro_batch_size_per_gpu * GPU number * gradient_accumulation_steps 

2. cpu offload adam报错
    
    可使用accelerate库替换，因为其封装了deepspeed，没有该问题。