1. Rho-1: 不是所有的标记都是你需要的

  标题：Rho-1: Not All Tokens Are What You Need

  机构：微软、厦门大学、清华大学

  关键词：语言模型、预训练、选择性训练、语料库

  作者：Zhenghao Lin, Zhibin Gou, Yeyun Gong

  分析：作者认为语言模型预训练方法过去都是对所有训练标记应用相同的下一个标记预测损失。作者挑战这一规范，指出“语料库中并非所有标记对语言模型训练同等重要”。作者的初步分析探讨了语言模型的标记级训练动态，揭示了不同标记的不同损失模式。利用这些观点，作者引入了一种名为Rho-1的新语言模型。与传统语言模型不同，Rho-1采用有选择性的语言建模（SLM），有选择地训练与期望分布相匹配的有用标记。这种方法涉及使用参考模型评分预训练标记，然后使用针对具有更高过度损失的标记的集中损失对语言模型进行训练。在持续预训练15B OpenWebMath语料库后，Rho-1在9个数学任务中的少样本准确率上取得了高达30%的绝对改进。精调后，Rho-1-1B和7B在MATH数据集上分别实现了40.6%和51.8%的最新结果，仅使用了3%的预训练标记就与DeepSeekMath匹敌。此外，在80B通用标记的预训练中，Rho-1在15个不同任务中实现了6.8%的平均增强，增加了语言模型预训练的效率和性能。

  地址：https://arxiv.org/pdf/2404.07965

# 参考

[1] 百度：GPT系列模型训练数据影响之探究 | 少样本准确率上提升30%，微软发布Rho-1 | 谷歌发布R-Gemma性能超越..， https://mp.weixin.qq.com/s/6aPtm-_e2ckQPJJ0lLZrwA
