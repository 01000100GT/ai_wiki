以两台机器为例，主机器称为node0，副机器称为node1，我都是在docker container里运行的，首先就是起container，把要运行代码所需的环境装好，这些略过不提。

除此之外，记得把两台机器container中的公钥添加到对方container的~/.ssh/authorized_keys中。

之后就比较常规了。在node0上运行：

```shell
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr="10.60.40.2" --master_port=9904 \
        supervised-fine-tune.py  \
        # 后面跟训练代码本身的参数，在此省略
```

`--nnnodes`指定节点（机器）数，--node_rank 指定指定当前node的id（0为master机器），`--master_addr填写master机器内网IP（公网IP应该也行）,`--master_port`绑定的端口号，自己指定即可。

然后在node1上运行（只需要改`--node_rank`为1即可，其他参数不变）：

```shell
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr="10.60.40.2" --master_port=9904 \
        supervised-fine-tune.py  \
        # 后面跟训练代码本身的参数，在此省略
```

# 参考

[1] torchrun多机多卡训练, https://zhuanlan.zhihu.com/p/662918505