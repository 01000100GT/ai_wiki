# 1. 开源项目

1. Firefly mixtral 8x7b
   - https://github.com/WangRongsheng/Aurora
   - https://huggingface.co/YeungNLP/firefly-mixtral-8x7b

2. LLaMA-MoE
   - 原有模型拆解为小的专家模型
   - LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training
   - https://github.com/pjlab-sys4nlp/llama-moe


