1. 智源评测

    评测说明：
    对于生成式对话模型，智源团队认为需要严格按照“模型在问题输入下自由生成的答案”进行评判，这种方式贴近用户真实使用场景，
    因此参考斯坦福大学HELM[1]工作进行评测，该评测对于模型的上下文学习和指令跟随能力要求更为严格。实际评测过程中，
    部分对话模型回答不符合指令要求，可能会出现“0”分的情况。例如：根据指令要求，正确答案为“A”，
    如果模型生成为“B”或“答案是 A ”，都会被判为“0”分。同时，业内也有其他评测方式，
    比如让对话模型先拼接“问题+答案”，模型计算各个拼接文本的概率后，验证概率最高的答案与正确答案是否一致，
    评测过程中对话模型不会生成任何内容而是计算选项概率。这种评测方式与真实对话场景偏差较大，因此在生成式对话模型评测中没有采纳。
    [1] https://crfm.stanford.edu/helm/latest

2. Firefly评测

人工评测

我们构建了评测集，其中包含13种评测任务，评测数据详见data/firefly-eval.xlsx。大部分数据从Belle数据中进行采样和优化。 
每种任务包含10条数据，一共130条数据。13种任务包含：头脑风暴、分类、Close QA、代码生成、 信息抽取、开放式生成、
有害性检验、数学题、阅读理解、Open QA、Rewrite、Summarization、翻译。

评测标准如下：

- 对于同一道题目，对两两模型的生成结果进行比较，存在胜负平三种关系。
- 对于客观题，如果两个模型均回答正确，或均回答错误，则为平局。
- 对于主观题，回答更加详细、真实、细节更丰富，则为获胜。当两者内容正确，并且详细程度非常接近时，或者各有千秋时，可视为平局。
- 对于中文题目，如果目标回复为中文，但模型却回复英文，则判为错误。


# 参考

[1] https://github.com/FlagAI-Open/Aquila2/blob/main/README_CN.md
[2] https://github.com/yangjianxin1/Firefly-LLaMA2-Chinese