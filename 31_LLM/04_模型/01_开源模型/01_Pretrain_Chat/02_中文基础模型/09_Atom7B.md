# 1. 简介

- 模型体验：https://llama.family/live
- 模型下载地址：https://huggingface.co/FlagAlpha/Atom-7B-Chat
- API获取：https://llama.family/docs/api
- 社区官网：https://llama.family/
- Github: https://github.com/FlagAlpha

本次升级主要包括以下三个方面：
- 训练数据量提升：本次更新后，目前Atom-7B模型多语种总训练数据量达2.7T tokens。
- 强化学习优化：社区标注了上万条人类偏好数据，训练了人类偏好的奖励模型。通过强化学习，
  大幅度提高了模型高质量结果输出的稳定性。 我们还在持续进行偏好数据标注，周期性地更新该模型。
- 上下文长度提升：从4K升级到32K。

社区还增加了百科、书籍、博客、新闻、公告、小说、金融数据、法律数据、医疗数据、代码数据、专业论文数据、
中文自然语言处理竞赛数据集等多种数据训练，总训练数据量达到2.7T。

# 2. 方法

奖励模型+强化学习优化

RLHF 主要分为三个步骤：
1. 预训练一个语言模型 (LM)。
2. 聚合问答偏好数据，并基于此训练一个奖励模型 (Reward Model，RM)。
3. 使用强化学习 (RL) 对 LM 进行微调。

![](.09_Atom7B_images/训练全流程.png)

虽然 RLHF 生成的模型效果提升显著，但 RLHF 管道比监督学习复杂得多，
涉及训练多个 LM 并在训练循环中从 LM 策略中采样，从而产生大量计算成本。

目前应用在大模型训练的主流的强化学习算法包括：DPO和PPO。

## 2.1. DPO

Direct Preference Optimization 直接偏好优化

通过在RLHF中使用新的奖励模型参数来简化微调过程，直接提取最优策略，使用简单的分类损失任务解决RLHF问题，从而避免了传统RLHF方法的复杂。

![](.09_Atom7B_images/DPO流程.png)

DPO的优点也比较明显，减少了RLHF的复杂度以及不稳定性，但同时泛化能力有限，
难以满足我们本次7B模型调优对于多种评判准则的要求，因此选择下面的PPO。

## 2.2. PPO

Proximal Policy Optimization Algorithms 近端策略优化

PPO是一种强化学习的策略梯度方法，与环境进行交替的交互采样数据，并使用随机梯度上升优化一个“替代”目标函数。
与执行每个数据样本一个梯度更新的标准策略梯度方法不同，PPO允许多个批次的小批量更新，也是ChatGPT选择的经典方法。

![](.09_Atom7B_images/训练全流程.png)

总结

近端策略优化（PPO）以其在策略梯度方法上的改进，为复杂任务和环境中的决策提供了更稳定和高效的解决方案。

直接偏好优化（DPO）则通过简化语言模型的微调过程，解决了强化学习中的复杂性和不稳定性问题。

但是由于社区对人类偏好的奖励模型评价标准的多样性，因此社区Atom-7B模型选择泛化性更好，更经典的PPO方法。

# 参考

[1] 社区Atom-7B模型重磅更新！免费下载可商用，https://mp.weixin.qq.com/s/dUpah18It6fyB2Jfm7jY9g