# 1. 介绍

论文：https://arxiv.org/pdf/2402.16141.pdf

![](.07_PeriodicLora_images/结构介绍.png)

LoRA 假设大模型微调优化过程本质上是低维的，通过在预训练的大型语言模型（LLMs）中引入额外的低秩矩阵来实现高效的微调。
然而，LoRA在微调过程中的权重更新受限于低秩矩阵，这导致了与全参数微调相比的性能差距。

PLoRA的核心思想是通过多次累积低秩更新矩阵来实现更高的更新秩。具体来说，PLoRA包含多个训练阶段，在每个阶段中，
仍然只更新LoRA权重。但在每个阶段结束时，将LoRA权重卸载到主干参数中，然后重新初始化LoRA状态，包括LoRA权重、相应的优化器状态和学习率调度器状态。理论上，PLoRA允许LoRA方法突破低秩更新矩阵的限制，接近全参数微调的效果。

特点如下：

1. 多阶段训练：PLoRA将下游任务的微调分为多个阶段，每个阶段都包含一定数量的步骤。

2. 累积更新：在每个阶段结束时，将训练得到的低秩矩阵乘积（BtAt）合并到主干参数矩阵W中，从而在多个阶段后获得高秩的最终更新。

3. 内存使用不变：尽管PLoRA通过累积低秩矩阵来增加更新的秩，但它并不增加内存使用量。

4. 动量卸载策略：为了缓解训练过程中的不稳定性，PLoRA引入了基于动量的卸载策略，通过选择性地更新LoRA权重，而不是完全替换它们。

5. 实验验证：通过在不同的PEFT设置中对LLaMA 7B模型进行指令微调，并在多主题多选题、数学推理以及语言理解和推理任务上评估性能，实验结果表明PLoRA在相同秩的情况下始终优于LoRA，且不引入额外的内存开销。

6. 超参:

    - LoRA的秩：作者尝试了不同秩的LoRA，包括1和8。实验结果表明，PLoRA在不同秩的设置下都能取得比传统LoRA更好的性能，尤其是在设置秩为8时。

    - 训练阶段的数量：PLoRA通过多个训练阶段来累积低秩更新矩阵，以实现更高的更新秩。作者通过实验发现，增加训练阶段的数量可以提高PLoRA的性能，但同时也可能导致过拟合。

    - 学习率：作者探讨了不同的学习率对PLoRA性能的影响。他们发现，较高的学习率在LoRA训练中更为合适，但过高的学习率可能会降低PLoRA的有效性。

    - 动量：为了缓解训练过程中的不稳定性，作者引入了基于动量的卸载策略。实验结果表明，适当的动量设置可以提高训练的稳定性，并在某些情况下降低训练损失。

    - 卸载点选择：作者讨论了在训练过程中何时卸载LoRA权重的问题。他们通过实验确定了最佳的卸载点，以确保训练的有效性和稳定性。

    - 应用LoRA的层：作者还探讨了在模型的不同线性层上应用LoRA的效果。他们发现，将LoRA应用于所有线性层（包括自注意力和多层感知器）可以取得更好的性能。

# 参考

[1] PeriodicLoRA：提高LoRA微调效果的又一个trick，https://mp.weixin.qq.com/s/i_wfckc_wD5JHLLbIJQTBw
