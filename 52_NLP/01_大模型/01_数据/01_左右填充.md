注：答案来自GPT-4o

在大模型训练中，**left padding**和**right padding**是两种常用的填充方式，它们分别用于在处理变长序列时填补空白位置，以保证输入的序列能够适配模型的固定输入长度。选择使用左侧填充（left padding）或右侧填充（right padding）取决于模型架构和训练任务的需求。下面是两者的差别及选择原因：

### 1. **Left Padding vs Right Padding**
   
- **Left Padding**：
  - **描述**：将填充符（通常是零或特定的padding值）放置在序列的**左侧**，即在序列的前面填充。
  - **使用场景**：常用于需要保持序列的顺序（如时间序列或自回归任务）时。例如，在机器翻译中，如果模型是自回归的（如Transformer Decoder），右侧的token必须生成，因此通常会采用左填充。
  - **优点**：
    - 对于一些自回归模型（如生成任务中的GPT系列模型），左侧填充可以使得模型能够先看到“有效”信息，避免填充部分影响生成。
    - 保持了更自然的训练顺序，尤其在句子生成任务中，模型从左到右逐词生成。

- **Right Padding**：
  - **描述**：将填充符放置在序列的**右侧**，即在序列的后面填充。
  - **使用场景**：许多模型，特别是编码器-解码器结构的模型（如BERT和T5等），通常会使用右侧填充，因为这种方式便于批量处理和序列对齐，且在推理时也不影响生成过程。
  - **优点**：
    - 右侧填充常常更简洁，适用于大多数任务，特别是需要对齐的任务，如分类、序列标注等。
    - 在输入序列长度不等时，填充通常放在末尾，不会影响前半部分的有效信息，便于批处理和并行计算。

### 2. **Padding值为-100的原因**

在处理标签（labels）时，使用`-100`作为填充值，通常是为了区分**有效标签**和**无效标签**，特别是在需要进行损失计算时。具体原因如下：

- **无效标签**：训练过程中，我们不希望计算填充部分的损失，因为它不对应任何有效的训练目标。因此，`-100`作为填充值可以明确表示该位置的标签是无效的，不应该被计算在损失函数中。
- **适应损失函数**：许多损失函数（如`CrossEntropyLoss`）支持忽略某些标签的损失计算。通常，`-100`用于标记那些应该忽略的填充位置。在这种情况下，`CrossEntropyLoss`等损失函数会跳过这些位置的损失计算，从而避免填充符对模型训练产生影响。

### 总结

- **Left padding** 和 **right padding** 的选择与模型架构和任务需求密切相关。自回归生成任务通常使用左侧填充，而编码器-解码器模型和分类任务更常用右侧填充。
- **-100** 作为padding值的主要目的是避免填充部分对损失计算的影响，确保模型只在有效部分进行学习。