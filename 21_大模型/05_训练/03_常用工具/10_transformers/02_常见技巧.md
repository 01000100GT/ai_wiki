# 1. 更快的模型加载

1. 使用automap快于to devcice
   
   - 可能由于to device需要将模型参数从cpu拷贝到gpu，而automap不需要，所以automap更快。
   - 实测llama2-13b模型，automap耗时23s，to device耗时80s
   
    ```python
    model = AutoModelForCausalLM.from_pretrained(model_dir,torch_dtype=torch.bfloat16).to(device)
    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True,
                                                 torch_dtype=torch.bfloat16).eval()  # faster than above 
    ```

2. 使用safe tensor快于bin文件

   qwen-72b模型的safetensor加载比llama2-13b快5秒

# 2. 模型多卡推理

1. 使用automap

   ```python
    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True,
                                                 torch_dtype=torch.bfloat16).eval()  # faster than above 
    ```