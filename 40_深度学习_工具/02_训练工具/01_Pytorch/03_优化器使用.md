以 Pytorch 框架为例，PyTorch 中所有的优化器(如: optim.Adadelta、optim.SGD、optim.RMSprop 等)
均是 Optimizer 的子类，Optimizer 中也定义了一些常用的方法:

- zero_grad(): 将梯度清零。
- step(closure): 执行一步权值更新, 其中可传入参数 closure(一个闭包)。
- state_dict(): 获取模型当前的参数，以一个有序字典形式返回，key 是各层参数名，value 就是参数。
- load_state_dict(state_dict): 将 state_dict 中的参数加载到当前网络，常用于模型 finetune。
- add_param_group(param_group): 给 optimizer 管理的参数组中增加一组参数，可为该组参数定制 lr, momentum, weight_decay 等，在 finetune 中常用。

优化器设置和使用的模板代码如下:

```python
# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 指定每一层的学习率
optim.SGD([
            {'params': model.base.parameters()},
            {'params': model.classifier.parameters(), 'lr': 1e-3}
          ], lr=1e-2, momentum=0.9
        )
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```