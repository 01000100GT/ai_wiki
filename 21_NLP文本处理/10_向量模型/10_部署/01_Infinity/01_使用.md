# 1. 资源

- Github (1.2k stars): https://github.com/michaelfeil/infinity
- 接口文档：https://michaelfeil.eu/infinity/0.0.54/swagger_ui/

特色：
- 支持动态batch
- 支持torch/onnx/tensorRT/optimum等多个后端

推理速度：
- cpu下推荐onnx最快
- gpu下推荐huggingface/TEI最快，但优化后的torch速度也不错
- 后端支持广泛：NVIDIA CUDA, AMD ROCM, CPU, AWS INF2 or APPLE MPS
- openAI embedding接口
- 支持embedding/rerank/classification/clip等模型
- 支持自动测速

问题：
- 例如，bge-m3 实测发现显存占用比直接用FlagEmbedding启动高，分别是2.7G和3.6G，不管batch设1还是2(可能预占用，为了推理加速?)
- 不管在传参配置fp16，还是在模型文件config.json中设为fp16，模型的显存占用都不变 (不确定原因)
- 不支持Jina embed v2 (可以启动，但是输出782维全是None，不确定原因)

# 2. 使用

## 2.1 安装

```bash
pip install infinity-emb[all]
```

## 2.2 CLI使用

参数查看

```bash
infinity_emb v2 --help
```

注意：在未完成启动时，尽量不要用Ctrl+C中断，否则可能停不下来

默认未热启动，会进行运行及测速

### 2.2.1 embedding

#### 2.2.1.1 服务端

**命令行启动**

cpu使用
```bash
infinity_emb v2 --model-id /root/mdl_zoo/Xorbits/bge-m3 --served-model-name embed --batch-size 1 --device cpu
```

gpu使用，通过compile和bettertransformer(flash attention)参数优化模型
```bash
infinity_emb v2 --model-id /root/mdl_zoo/Xorbits/bge-m3 --served-model-name embed --batch-size 2 --device cuda --compile --bettertransformer
```

**参数说明**

输入参数：
```json
{
  "input": [
    "string"
  ],
  "model": "default/not-specified",
  "user": "string"
}
```

返回参数：

code 200

```json
{
  "object": "embedding",
  "data": [
    {
      "object": "embedding",
      "embedding": [
        0
      ],
      "index": 0
    }
  ],
  "model": "string",
  "usage": {
    "prompt_tokens": 0,
    "total_tokens": 0
  },
  "id": "string",
  "created": 0
}
```

#### 2.2.1.2 客户端

通过openai接口

```python
from openai import OpenAI

base_url = "http://0.0.0.0:7997"
client = OpenAI(api_key="EMPTY", base_url=base_url)


def embedding():
    response = client.embeddings.create(
        model="embed",
        input=["样例数据-1", "样例数据-2"],
        #input=["你好，给我讲一个故事，大概100字"],
    )
    embeddings = response.data[0].embedding
    print("嵌入完成，维度：", len(embeddings))
    print(embeddings)


if __name__ == "__main__":
    embedding()
```

通过post请求

```python
import requests
requests.post("http://..:7997/embeddings",
    json={"model":"BAAI/bge-small-en-v1.5","input":["A sentence to encode."]})
```

### 2.2.2 rerank

#### 2.2.2.1 服务端

**命令行启动**

gpu使用，通过compile和bettertransformer(flash attention)参数优化模型
```bash
infinity_emb v2 --model-id /root/mdl_zoo/AI-ModelScope/bge-reranker-v2-m3 --served-model-name rerank --batch-size 2 --device cuda --compile --bettertransformer
```

**参数说明**

输入传参：
```json
{
  "query": "string",
  "documents": [
    "string"
  ],
  "return_documents": false,
  "model": "default/not-specified"
}
```

返回参数：

```json
{
  "object": "rerank",
  "results": [
    {
      "relevance_score": 0,
      "index": 0,
      "document": "string"
    }
  ],
  "model": "string",
  "usage": {
    "prompt_tokens": 0,
    "total_tokens": 0
  },
  "id": "string",
  "created": 0
}
```

#### 2.2.2.2 客户端

```python
import requests
requests.post("http://..:7997/rerank",
    json={
        "model":"mixedbread-ai/mxbai-rerank-xsmall-v1",
        "query":"Where is Munich?",
        "documents":["Munich is in Germany.", "The sky is blue."]
    })
```

## 2.3 Langchain使用

参考：https://python.langchain.com/v0.1/docs/integrations/text_embedding/infinity/

注意：如果下面代码无法连接，则把url中的v1删除

```python
from langchain_community.embeddings import InfinityEmbeddings, InfinityEmbeddingsLocal

documents = [
    "Baguette is a dish.",
    "Paris is the capital of France.",
    "numpy is a lib for linear algebra",
    "You escaped what I've escaped - You'd be in Paris getting fucked up too",
]
query = "Where is Paris?"

#
infinity_api_url = "http://localhost:7797/v1"
# model is currently not validated.
embeddings = InfinityEmbeddings(
    model="sentence-transformers/all-MiniLM-L6-v2", infinity_api_url=infinity_api_url
)
try:
    documents_embedded = embeddings.embed_documents(documents)
    query_result = embeddings.embed_query(query)
    print("embeddings created successful")
except Exception as ex:
    print(
        "Make sure the infinity instance is running. Verify by clicking on "
        f"{infinity_api_url.replace('v1','docs')} Exception: {ex}. "
    )

# (demo) compute similarity
import numpy as np

scores = np.array(documents_embedded) @ np.array(query_result).T
dict(zip(documents, scores))
```

输出
```json
{'Baguette is a dish.': 0.31344215908661155,
 'Paris is the capital of France.': 0.8148670296896388,
 'numpy is a lib for linear algebra': 0.004429399861302009,
 "You escaped what I've escaped - You'd be in Paris getting fucked up too": 0.5088476180154582}
```

# 3. 测速评价

Results: CPU-only (BAAI/bge-small-en-v1.5 | bert-small)

```markdown
| Model                            | Time (seconds) | Requests # / sec (mean)           |
|----------------------------------|----------------|-----------------------------------|
| infinity-optimum-int8            | 100.490        | 0.10                              |
| infinity-optimum (onnx)          | 125.342        | 0.08                              |
| fastembed (onnx)                 | 125.770        | 0.08                              |
| sentence-transformers (torch)    | 256.884        | 0.04                              |
| infinity (torch)                 | 353.065??      | 0.03 (needs revision)             |
| huggingface/TEI (candle)         | 1104.357       | 0.009                             |
```

Results: NVIDIA L4 (BAAI/bge-large-en-v1.5 | bert-large)

```markdown
| Model                                       | Requests # / sec (mean) | Time (seconds) |
|---------------------------------------------|-------------------------|----------------|
| huggingface/TEI (candle, flashbert)         | 0.54                    | 18.491         |
| infinity (torch + compile + fa2)            | 0.51                    | 19.562         |
| tensorrt (via infinity)                     | 0.43                    | 23.367         |
| infinity (onnx-gpu fp16, fused layers)      | 0.41                    | 24.448         |
| sentence-transformers (fp16)                | 0.17                    | 59.107         |
```