1. InternLM开源框架
   - https://github.com/InternLM/xtuner
     - 支持各类大模型训练
   - https://github.com/InternLM/InternEvo
     - 支持internLM的pretrain和sft大规模训练
     - 在多节点通讯和数据打包上做了优化

2. FlagScale
    - Github (121 stars): https://github.com/FlagOpen/FlagScale
    - 数据Padding做了pack优化，训练速度减半
    - 支持megatron-lm、vllm
    - 支持异构硬件训练

3. NeMo
    - Github (11.3k stars): https://github.com/NVIDIA/NeMo
    - 英伟达开源
    - 各类训练加速策略

4. Megatron-LM
   - Github (11.3k stars): https://github.com/NVIDIA/Megatron-LM
   - 英伟达开源
   - 各类训练加速策略