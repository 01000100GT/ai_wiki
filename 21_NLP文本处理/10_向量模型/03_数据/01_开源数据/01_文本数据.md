# 1. llm-embedder
- 下载：https://huggingface.co/datasets/namespace-Pt/projects/resolve/main/llm-embedder.tar.gz
- 大小15.7G
- Paper: Retrieve Anything To Augment Large Language Models
- 基于大模型过程中常用的6个任务微调:
  - Question Answering (qa)
  - Conversational Search (convsearch)
  - Long Conversation (chat)
  - Long-Range Language Modeling (lrlm)
  - In-Context Learning (icl)
  - Tool Learning (tool)

# 2. baai_general_embedding微调训练数

t2ranking, dulreader, mmarco, cmedqav2, mulit-cpr, nli-zh, ocmnli, and cmnli. For t2ranking, dulreader, and mmarco, we mine hard negatives; For nli-zh, ocmnli, and cmnli, we use the pairs whose label equal to 0 as negatives; For cmedqav2 and mulit-cpr, we randomly sample negatives.

# 3. BAAI-MTP

- 数据下载：https://data.baai.ac.cn/details/BAAI-MTP
- 北京智源人工智能研究院信息检索与知识计算组构建并对外发布数据集MTP（Massive Text Pairs）。MTP由总计3亿条中英文关联文本对构成；其中，中文记录达1亿条，英文数据达2亿条。MTP为迄今开源的最大规模中英文关联文本对数据集，为训练中英文语义向量模型提供了重要的基础。
- 数据构成
  - ![](.01_文本数据_images/BAAI-MTP数据构成.png)
  - ![](.01_文本数据_images/新增数据.png)
- 论文：
  - RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder
  - RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models