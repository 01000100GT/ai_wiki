1. deepspeed配置不一致问题
    
    特别在开启zero3时，提示deepspeed参数配置和训练传入的参数不一致，需要保持一致。
    其他参数可以射程‘auto’以自动获取启动脚本中的传参。
    
   ```json
     "train_batch_size": 24,
     "train_micro_batch_size_per_gpu": 1,
     "gradient_accumulation_steps": 1,
      "bf16": {
       "enabled": false
        },
        "fp16": {
          "enabled": true,
          "loss_scale": 0,
          "loss_scale_window": 100
        },
   ```
   
   - 如果启动脚本中加入--fp16, deepspeed中需要设置true或者auto 
   - 集群启动时，特别在使用zero3时，不支持设置auto，train_batch_size = 
     train_micro_batch_size_per_gpu * GPU number * gradient_accumulation_steps 

2. cpu offload adam报错
    
    可使用accelerate库替换，因为其封装了deepspeed，没有该问题。

3. zero3模型无法正常保存
    
    如下方法尚未进行测试验证，仅验证了MOSS的zero3可以正常保存（基于accelerate库写的）    

    参考：
    - https://github.com/huggingface/peft/issues/453
    - https://github.com/huggingface/peft/issues/460

    方法1： 改写trainer中save_checkpoint

    ```python
     from peft import get_peft_model_state_dict
    
     class MyTrainer(Seq2SeqTrainer):
    
        def _save_checkpoint(self, _, trial, metrics=None):
            """ Don't save base model, optimizer etc.
                but create checkpoint folder (needed for saving adapter) """
            checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}"
    
            run_dir = self._get_output_dir(trial=trial)
            output_dir = os.path.join(run_dir, checkpoint_folder)
    
            if metrics is not None and self.args.metric_for_best_model is not None:
                metric_to_check = self.args.metric_for_best_model
                if not metric_to_check.startswith("eval_"):
                    metric_to_check = f"eval_{metric_to_check}"
                metric_value = metrics[metric_to_check]
    
                operator = np.greater if self.args.greater_is_better else np.less
                if (self.state.best_metric is None or self.state.best_model_checkpoint is None
                        or operator(metric_value, self.state.best_metric)):
                    self.state.best_metric = metric_value
    
                    self.state.best_model_checkpoint = output_dir
    
            os.makedirs(output_dir, exist_ok=True)
    
            if self.args.should_save:
                self._rotate_checkpoints(use_mtime=True, output_dir=run_dir)
    
            # save adapter config
            self.model.peft_config.save_pretrained(output_dir)
            # get state dict through deepspeed engine
            engine_state_dict = self.model_wrapped._zero3_consolidated_16bit_state_dict()
            lora_state_dict = get_peft_model_state_dict(self.model, engine_state_dict)
            if self.args.local_rank == 0:
                torch.save(lora_state_dict, os.path.join(output_dir, "adapter_model.bin"))
                print(f"Save adapter model at {output_dir}")
    ```
    
    方法2：

    ```python
    lora_model = PeftModel.from_pretrained(
                base_model,
                output_dir,
                torch_dtype=torch.float16
            )
    
    model = lora_model.merge_and_unload()
   ```