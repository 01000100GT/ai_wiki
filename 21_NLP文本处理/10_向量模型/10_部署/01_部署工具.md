# 1. infinity

资料：
- Github (1.1k stars): https://github.com/michaelfeil/infinity
- 使用文档：https://michaelfeil.eu/infinity/0.0.53/
- Langchain集成：https://python.langchain.com/v0.1/docs/integrations/text_embedding/infinity/

功能：
- 支持动态batch
- 支持onnx及torch模型

# 2. torch-npu

昇腾NPU 910A 测试使用固定长推理，变成推理，每次一变，速度就很慢

凡是支持torch的模型，简单修改就可以使用，另外，像FlagEmbedding中的bge-m3的embedding和bge-m3-v2-reranker都支持了NPU推理

但实际测试发现，一旦输入变长，首次推理速度会很慢，但如果固定长推理，第二次开始会很快

实际测试，自己直接按torch-npu导致推理失败，可能是某些依赖安装错误，可以使用[llama-factory](https://github.com/hiyouga/LLaMA-Factory.git)

```shell
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```

# 3. Xinference

- 仅支持torch版本模型
- 支持多节点部署

# 4. fast-embed
- 支持onnx，速度更快
- 不确定是否提供client方式