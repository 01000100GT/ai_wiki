大语言模型微调

最近 OpenAI 开始提供大语言模型微调 API，LlamaIndex 也有关于在 RAG 设置中微调 GPT-3.5-turbo 的教程，
旨在“提炼”GPT-4 的部分知识。这里的想法是获取一个文档，用 GPT-3.5-turbo 生成一些问题，然后使用 GPT-4 
根据文档内容生成这些问题的答案（构建一个由 GPT-4 驱动的 RAG 流水线），接着在这个问题-答案对数据集上微调 
GPT-3.5-turbo。用于 RAG 流水线评估的 ragas 框架显示了 5% 的忠实度指标提升，意味着微调后的 GPT-3.5-turbo 
模型比原始模型更好地利用提供的上下文生成答案。

一种更复杂的方法在最近的论文《RA-DIT: Retrieval Augmented Dual Instruction Tuning》中展示，
提出了一种同时调整大语言模型和检索器（原论文中的双编码器）的技术，基于查询、上下文和答案的三元组。关于实现细节，
请参考这个指南。这种技术用于通过微调 API 微调 OpenAI 大语言模型，也用于微调开源 Llama2 模型（在原论文中），
在知识密集型任务指标上提升了约 5%（与搭载 RAG 的 Llama2 65B 相比），在常识推理任务上也有几个百分点的提升。
