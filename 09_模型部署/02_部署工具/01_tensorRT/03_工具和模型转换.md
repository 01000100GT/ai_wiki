# 1. 简介
&emsp;&emsp;Git网址：https://github.com/NVIDIA/TensorRT   
&emsp;&emsp;将torch和tensorflow(如下简称tf)或keras模型转换成tensorRT(如下简称TRT)，可以在GPU上加速推理。虽然TRT已经推出了好几年，
目前已经到了8.5版本，目前仍有不少torch和tf的算子存在不支持问题。可以用的模型有如下几种途径：
* TF(Keras)->TRT
    * 通过TF自带工具，直接转换 (需要注意版本匹配问题，不能自如切换TensorRT各种版本)
        * 文档：https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-precision-levels
        * 样例：https://github.com/tensorflow/tensorrt
        * 优点：（1）可能可以规避算子不支持问题，且优化的更好（未进行测试） （2）TF自带，无需额外安装，无需源码编译
        * 缺点：文档不是非常详细，对于多输入多输出，以及动态维度范围定义描述不详细。设定层的数据类型不确定如何设置。
    * 通过TF->ONNX->TRT：ONNX虽然也发展了很久，依然存在不支持算子
* Torch->TRT
    * 通过Torch-TRT工具转换
        * Git网址：https://github.com/pytorch/TensorRT
        * 优点：可能可以规避算子不支持问题，且优化的更好（未进行测试）
        * 缺点：对TRT版本要求较高，且编译较为复杂
    * 通过Torch->ONNX->TRT
* ONNX->TRT
    * 该工具已在TensorRT的工具包里集成，无需单独安装，单独安装很容易出现编译问题

&emsp;&emsp;转换工具介绍如下。

# 2. trexe  
&emsp;&emsp;trexe是一个CLI模型转换及分析的工具，如下是基于8.5版本。该工具位于预编译版本TensorRT-8.5.1.7\bin\trtexec.exe
* 文档：https://docs.nvidia.cn/deeplearning/tensorrt/developer-guide/index.html#trtexec
* 介绍样例：在Git源码中samples\trtexec

## 2.1 参数说明
&emsp;&emsp;CLI对应参数如下：     
Flags for the Build Phase   
* --onnx=<model>: Specify the input ONNX model.
* --deploy=<caffe_prototxt>: Specify the input Caffe prototxt model.
* --uff=<model>: Specify the input UFF model.
* --output=<tensor>: Specify output tensor names. Only required if the input models are in UFF or Caffe formats.
* --maxBatch=<BS>: Specify the maximum batch size to build the engine with. Only needed if the input models are in UFF or Caffe formats. If the input model is in ONNX format, use the --minShapes, --optShapes, --maxShapes flags to control the range of input shapes including batch size.
* --minShapes=<shapes>, --optShapes=<shapes>, --maxShapes=<shapes>: Specify the range of the input shapes to build the engine with. Only required if the input model is in ONNX format.
* --workspace=<size in MB>: Specify the maximum size of the workspace that tactics are allowed to use. This flag has been deprecated. You can use the –-memPoolSize=<pool_spec> flag instead.
* –-memPoolSize=<pool_spec>: Specify the maximum size of the workspace that tactics are allowed to use, as well as the sizes of the memory pools that DLA will allocate per loadable.
* --saveEngine=<file>: Specify the path to save the engine to.
* --fp16, --int8, --noTF32, --best: Specify network-level precision.
* --sparsity=[disable|enable|force]: Specify whether to use tactics that support structured sparsity.
    disable: Disable all tactics using structured sparsity. This is the default.
    enable: Enable tactics using structured sparsity. Tactics will only be used if the weights in the ONNX file meet the requirements for structured sparsity.
    force: Enable tactics using structured sparsity and allow trtexec to overwrite the weights in the ONNX file to enforce them to have structured sparsity patterns. Note that the accuracy is not preserved, so this is to get inference performance only.
* --timingCacheFile=<file>: Specify the timing cache to load from and save to.
* --verbose: Turn on verbose logging.
* --buildOnly: Build and save the engine without running inference.
* --profilingVerbosity=[layer_names_only|detailed|none]: Specify the profiling verbosity to build the engine with.
* --dumpLayerInfo, --exportLayerInfo=<file>: Print/Save the layer information of the engine.
* --precisionConstraints=spec: Control precision constraint setting.
    none: No constraints.
    prefer: Meet precision constraints set by --layerPrecisions/--layerOutputTypes if possible.
    obey: Meet precision constraints set by --layerPrecisions/--layerOutputTypes or fail otherwise.
* --layerPrecisions=spec: Control per-layer precision constraints. Effective only when precisionConstraints is set to obey or prefer. The specs are read left to right, and later ones override earlier ones. "*" can be used as a layerName to specify the default precision for all the unspecified layers.
    For example: --layerPrecisions=*:fp16,layer_1:fp32 sets the precision of all layers to FP16 except for layer_1, which will be set to FP32.
* --layerOutputTypes=spec: Control per-layer output type constraints. Effective only when precisionConstraints is set to obey or prefer. The specs are read left to right, and later ones override earlier ones. "*" can be used as a layerName to specify the default precision for all the unspecified layers. If a layer has more than one output, then multiple types separated by "+" can be provided for this layer.
    For example: --layerOutputTypes=*:fp16,layer_1:fp32+fp16 sets the precision of all layer outputs to FP16 except for layer_1, whose first output will be set to FP32 and whose second output will be set to FP16.
* –useDLACore=N: Use the specified DLA core for layers that support DLA.
* –allowGPUFallback: Allow layers unsupported on DLA to run on GPU instead.

Flags for the Inference Phase
* --loadEngine=<file>: Load the engine from a serialized plan file instead of building it from input ONNX, UFF, or Caffe model.
* --batch=<N>: Specify the batch size to run the inference with. Only needed if the input models are in UFF or Caffe formats. If the input model is in ONNX format or if the engine is built with explicit batch dimension, use --shapes instead.
* --shapes=<shapes>: Specify the input shapes to run the inference with.
* --warmUp=<duration in ms>, --duration=<duration in seconds>, --iterations=<N>: Specify the minimum duration of the warm-up runs, the minimum duration for the inference runs, and the minimum iterations of the inference runs. For example, setting --warmUp=0 --duration=0 --iterations allows users to control exactly how many iterations to run the inference for.
* --useCudaGraph: Capture the inference to a CUDA graph and run inference by launching the graph. This argument may be ignored when the built TensorRT engine contains operations that are not permitted under CUDA graph capture mode.
* --noDataTransfers: Turn off host to device and device-to-host data transfers.
* --streams=<N>: Run inference with multiple streams in parallel.
* --verbose: Turn on verbose logging.
* --dumpProfile, --exportProfile=<file>: Print/Save the per-layer performance profile.

Refer to trtexec --help for all the supported flags and detailed explanations.
Refer to GitHub: trtexec/README.md file for detailed information about how to build this tool and examples of its usage.

## 2.2 模型优化
1. 数据搬运时间优化
    &emsp;&emsp;如果Throughput或Enqueue Time时间远低于GPU Compute时间，GPU利用率低，可能由于host繁忙或数据搬运。
    Using CUDA graphs (with --useCudaGraph) or disabling H2D/D2H transfers (with --noDataTransfer) 
    may improve GPU utilization.
![](.01_简介及文档资料_images/TensorRT耗时分析图.png)

## 2.3 性能分析
* --dumpProfile到处模型每层耗时（需要CUDA 11.1以上版本）。 
* --profilingVerbosity=detailed,查看更详细的引擎信息和数据绑定信息。

# 3. Polygraph
## 3.1 介绍
&emsp;&emsp;Polygraph是TRT提供的调试分析工具，可以运行ONNX和TRT模型，精度对比分析。
* 优点：
    * 相比TRT+PyCuda运行TRT模型，调用更加简洁，且无需安装PyCUDA(容易出现安装失败的问题)
    * 提供了Python接口，和CLI（命令行工具）工具
* 缺点：
    * 除了ONNX和TRT精度对比的功能较为有用，其他的都可以用TRT+PyCuda或trtexe工具取代
    * 不确定是否可以取代TRT+PyCuda进行部署使用

## 3.2 安装
&emsp;&emsp;该工具位于TensorRT的Git仓中,tools\Polygraphy，如下安装方法2选1：
* 预编译whl安装（该方法无法安装CLI工具，请使用源码编译安装）
    ```bash
    python -m pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com
    ```
* 编译安装
    * Windows需要开启命令行执行权限，默认不允许执行，百度如何开启，使用Powershell执行
    ```bash
      .\install.ps1
    ```
    * Linux
    ```bash
      make install
    ```