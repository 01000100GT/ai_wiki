# 1. 中文

## 1.1 通用领域

1. Wikipedia: 中文Wikipedia的数据
   - https://github.com/goldsmith/Wikipedia
   - 2.7k stars
   - 提供python API

2. 悟道（智源）
   - 5T, 开源200G
   - 采用 20 多种规则从 100TB 原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免 GPT-3 
     存在的隐私泄露风险；包含教育、科技等 50+个行业数据标签，可以支持多领域预训练模型的训练
   - https://data.baai.ac.cn/details/WuDaoCorporaText
   - https://github.com/yuanzhoulvpi2017/zero_nlp/wiki/%E6%95%B0%E6%8D%AE%E5%85%B1%E4%BA%AB%E2%80%94%E2%80%94%E6%82%9F%E9%81%93200G%E6%95%B0%E6%8D%AE%E5%88%86%E4%BA%AB

3. MNBVC
   - https://github.com/esbatmop/MNBVC
   - 2k stars
   - MNBVC(Massive Never-ending BT Vast Chinese corpus)超大规模中文语料集。
     对标chatGPT训练的40T数据。MNBVC数据集不但包括主流文化，
     也包括各个小众文化甚至火星文的数据。MNBVC数据集包括新闻、作文、小说、
     书籍、杂志、论文、台词、帖子、wiki、古诗、歌词、商品介绍、笑话、糗事、
     聊天记录等一切形式的纯文本中文数据。
   - 包含数据清洗工具

4. 书生·万卷1.0
    - 地址：https://arxiv.org/pdf/2308.10755.pdf
    - 包含多种模式的大规模训练语料库：书生·万卷文本数据集1.0由来自网页、百科、书籍、专利、教材、
      考题等不同来源的清洗后预训练语料组成，数据总量超过5亿个文档，数据大小超过1TB
    - 该语料将html、text、pdf、epub等多种格式的数据统一处理为字段统一的jsonl格式，并经过细粒度的清洗、去重、价值对齐，
      文本数据包括6亿多篇文档，数据存储量超过1TB；图像-文本数据经过处理后成为文档，共有2200多万篇文档，
      数据量超过200GB（图像通过URL链接提供）；视频文件共有1000多个，数据量超过900GB。
    - 中文数据占比35.1%，约2.2亿个文件，466.54GB。英文数据集占比61.4%，共3.83亿个文件，542.51GB
    - 采用多步骤文本提取流程、语言检测、语料库过滤和重复数据删除
    - ![](.01_预训练数据_images/书生数据构成.png)

     在数据存储上，提供统一的JSON格式，数据样例如下：
     
    ```{
    "id": "BkORdv3xK7IA0HG7pccr",
    "content": "\\*诗作[222]\n录自索菲娅·马克思的笔记本\n#### 人生\n时光倏忽即逝，\n宛如滔滔流水；\n时光带走的一切，\n永远都不会返回。\n生就是死，\n生就是不断死亡的过程；"
    }
    ```
 
5. CCI中文互联网语料库
    - 在数据来源上均为高质量可信的、中国境内的互联网站，经过严格的数据清洗和去重，
      并在内容质量、价值观等方面进行了针对性的检测和过滤，进一步提升数据质量和安全可信程度。数据处理的规则包括：
      - 基于规则的过滤：文字密度提取、关键词过滤、垃圾信息过滤、简繁体转换等
      - 基于模型的过滤：低质量内容过滤
      - 数据去重：数据集内部 / 数据集间去重
    - CCI语料库首期开放的数据（CCI v1.0.0）规模为 104GB。数据集总体的时间跨度为 2001年1月至2023年11月
    - https://data.baai.ac.cn/details/BAAI-CCI

6. 安娜的檔案
   - ***强烈推荐***
   - 世界最大的图书馆，包含了大量的文本数据
   - 其中书籍类，有重复数据，可用种子，并发下载
   - https://tw.annas-archive.org/

7. pleisto/wikipedia-cn-20230720-filtered
   - https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered
   - Wiki中文百科（25w词条）
   - 500MB
   - ![](.01_预训练数据_images/wiki中文数据图例.png)

8. BaiduBaiKe
   - 563w词条
   - 百度网盘：https://pan.baidu.com/share/init?surl=jIpCHnWLTNYabftavo3DVw&pwd=bwvb  ， 提取码：bwvb
   - 4.5G

9. ChineseWebText
   - Github: https://github.com/CASIA-LM/ChineseWebText
   - Data: https://huggingface.co/datasets/CASIA-LM/ChineseWebText
   - Data Intro:  1.42 TB all, 600G cleaned
   
   ![](.01_预训练数据_images/ChineseWebText介绍.png)

   ```json
   {
     "title": "潍坊银行2021年上半年净利润同比增长29.57% 不良率降至1.10%_财经_中国网",
     "score": 0.95,
     "text": "潍坊银行2021年上半年净利润同比增长29.57% 不良率降至1.10%\n中国网财经8月24日讯 潍坊银行昨日披露2021年二季度信息报告显示，截至2021年6月末，潍坊银行资产总额1920.44亿元，较上年末增长9.34%；负债总额1789.16亿元，较上年末增长10.54%。2021年上半年，潍坊银行实现净利润6.09亿元，同比增长29.57%。\n资产质量方面，截至2021年6月末，潍坊银行不良贷款率1.10%，较上年末下降0.13个百分点。\n资本金方面，截至2021年6月末，潍坊银行资本充足率、核心一级资本充足率、一级资本充足率分别为11.66%、7.89%、10.13%，分别较上年末下降1.89、0.89、1.15个百分点。",
     "url": "http://finance.china.com.cn/news/special/2021bnb/20210824/5638343.shtml",
     "source_domain": "finance.china.com.cn"
   }
   ```
   
   - "title": 【string】The title of the data text.
   - "score": 【float】Quality score generated by the quality evaluation model.
   - "text": 【string】Text content of data sample.
   - "url": 【string】External URL, points to the original web address of the text.
   - "source_domain": 【string】The domain name of the source website.

10. 电信
    - 开源项目链接地址：1T高质量数据 
    - https://gitee.com/Tele-AI/tele-chat
    - https://github.com/Tele-AI/Telechat 
   
11. 整理好的wiki
    - https://github.com/brightmart/nlp_chinese_corpus?tab=readme-ov-file
    - 8.9k Stars
    - 数据集类别
      - 1.维基百科(wiki2019zh)，100万个结构良好的中文词条
      - 2.新闻语料(news2016zh)，250万篇新闻，含关键词、描述
      - 3.百科问答(baike2018qa)，150万个带问题类型的问答
      - 4.社区问答json版(webtext2019zh)，410万个高质量社区问答，适合训练超大模型
      - 5.翻译语料(translation2019zh)，520万个中英文句子对

12. 各类数据全面收录Chatterbox
    - 127 Stars
    - https://github.com/enze5088/Chatterbox
    - 主要搜集预训练数据集、指令微调数据集

13. Firefly继续预训练数据
    - Huggingface: https://huggingface.co/datasets/YeungNLP/firefly-pretrain-dataset/tree/main
    - Modelscope下载存在问题，建议使用Huggingface下载
    - 一共约22GB文本，主要包含CLUE、ThucNews、CNews、COIG、维基百科等开源数据集，以及我们收集的古诗词、散文、文言文等，数据分布如下图
    - ![](.01_预训练数据_images/数据分布.png)
    - 数据样例
    - ![](.01_预训练数据_images/数据样式.png)
    - Github: https://github.com/yangjianxin1/Firefly-LLaMA2-Chinese

14. 序列猴子
    - 出门问问开源
    - 介绍：https://github.com/mobvoi/seq-monkey-data/blob/main/docs/pretrain_open_corpus.md
    - 下载：http://share.mobvoi.com:5000/sharing/O91blwPkY
    - 通用文本数据集由来自网页、百科、博客、问答、开源代码、书籍、报刊、专利、教材、考题等多种公开可获取的数据
      进行汇总清洗之后而形成的大语言模型预训练语料。
    - 开源数据集（以下简称序列猴子数据集）是从序列猴子通用文本数据集的中文数据集中抽取 13,000,000 份数据而得到。

15. 北京700TB通用数据集
    - 通用数据集为用于通用基础模型训练的多种模态数据。当前已经汇聚在数据运营平台的通用数据集有116个，总数据量700.27TB，其中文本数据9.76TB，多模态图文数据量75.31TB，视频数据量615TB，音频数据0.2TB
    - 行业数据集包含了行业领域特有的知识和信息，用于训练各种行业模型，推动人工智能从通用向专业化、精细化持续发展。目前行业专区数据集28个，数据量4.33TB，其中文本数据集22个，数据量4.3TB，多模态图文行业数据集6个，数据量0.03TB。
    - ![](.01_预训练数据_images/数据详情.png)

## 1.2 专有领域

1. 医疗类数据
   - https://huggingface.co/datasets/shibing624/medical
   - 预训练362k条，约1G
    ![](.01_预训练数据_images/医疗预训练中文数据.png)
   - 微调2.07M条
     ![](.01_预训练数据_images/医疗微调中文数据.png)
   - 奖励模型4k条
     ![](.01_预训练数据_images/奖励模型.png)


# 2. 英文

1. Pile
   - 论文：https://arxiv.org/abs/2101.00027
   - Github: https://github.com/EleutherAI/the-pile
   - 825GB
   - ![](.01_预训练数据_images/pile数据构成.png)
   - ![](.01_预训练数据_images/详细分布.png)

2. Redpajama
   - https://www.together.ai/blog/redpajama
   - 复刻llama最好的数据
   - 1.2 T tokens
   - 数据来源
     - CommonCrawl: Five dumps of CommonCrawl, processed using the CCNet pipeline, 
        and filtered via several quality filters including a linear 
        classifier that selects for Wikipedia-like pages.
     - C4: Standard C4 dataset
     - GitHub: GitHub data, filtered by licenses and quality
     - arXiv: Scientific articles removing boilerplate
     - Books: A corpus of open books, deduplicated by content similarity
     - Wikipedia: A subset of Wikipedia pages, removing boilerplate
     - StackExchange: A subset of popular websites under StackExchange, removing boilerplate
      
     ![](.01_预训练数据_images/redpajama数据详情.png)
   
3. FinWeb
    - FineWeb 由 huggingface 领导的团体研发，提供超过15万亿个Token，这些Token来自2013年至2024年的 CommonCrawl转储
    - ![](.01_预训练数据_images/数据处理流程.png)
    - FineWeb在设计时一丝不苟，使用datatrove进行流水线处理。这个过程针对数据集进行清理和重复数据删除的操作，从而提高其质量和适用性以便利于大语言模型的训练和评估。
    - FineWeb的主要优势之一在于其性能。通过精心策划和创新的过滤技术，FineWeb在各种基准测试任务中优于C4、Dolma v1.6、The Pile和 SlimPajama 等已建立的数据集。在FineWeb上训练的模型表现出卓越的性能，它已经成为自然语言处理的宝贵资源。
    - 该数据集及其处理管道代码在ODC-By 1.0许可下发布
    - FineWeb利用了URL 过滤、语言检测和质量评估等过滤步骤提高数据集的完整性和丰富性。每个CommonCrawl转储都使用高级MinHash技术单独删除重复数据，进一步提高了数据集的质量和实用性。

# 参考

[1] 大模型研发必备：两大开源可用且清洗过的中文文本语料库及大模型FLOPS、参数量快速估计工具推荐，
    https://mp.weixin.qq.com/s/uqU0LKzchGLmXiDswRFkrQ