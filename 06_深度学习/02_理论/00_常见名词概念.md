1. 梯度下降  
    梯度下降是用于查找函数最小值的一阶迭代优化算法。 要使用梯度下降找到函数的局部最小值，
    可以采用与当前点的函数梯度（或近似梯度）的负值成比例的步骤。 
    如果采取的步骤与梯度的正值成比例，则接近该函数的局部最大值，被称为梯度上升。
    
    来源： Vapnik V. N. (2000). The Nature of Statistical Learning Theory. 
    Information Science and Statistics. Springer-Verlag.Wikipedia
    
2. 优化器  
    优化器基类提供了计算梯度loss的方法，并可以将梯度应用于变量。
    优化器里包含了实现了经典的优化算法，如梯度下降和Adagrad。 
    优化器是提供了一个可以使用各种优化算法的接口，可以让用户直接调用一些经典的优化算法，
    如梯度下降法等等。优化器（optimizers）类的基类。
    这个类定义了在训练模型的时候添加一个操作的API。用户基本上不会直接使用这个类，
    但是你会用到他的子类比如GradientDescentOptimizer, AdagradOptimizer, 
    MomentumOptimizer（tensorflow下的优化器包）等等这些算法。
    
    来源： 维基百科