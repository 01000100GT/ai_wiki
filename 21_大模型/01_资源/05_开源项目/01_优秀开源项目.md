# 1. LLM
## 1.1 入门项目

1. baby-llama2-chinese
    - 用于从头预训练+SFT一个小参数量的中文LLaMa2的仓库；24G单卡即可运行得到一个具备简单中文问答能力的chat-llama2.
    - https://github.com/DLLXW/baby-llama2-chinese
    - 899 stars

## 1.2 模型训练框架


## 1.3 综合训练框架

1. LLaMA-Factory
   - 简介：支持多种不同模型，支持pretrain、sft、ppo、RLHF各流程，较全面
   - https://github.com/hiyouga/LLaMA-Factory
   - 7.6 Stars

2. ChatGPT原理与实战：大型语言模型的算法、技术和私有化
   - https://github.com/liucongg/ChatGPTBook
   - 167 Stars
   - 公众号：刘聪NLP 
   - 简介：包含pretrain、sft、ppo、RLHF各流程

## 1.4 MOE

1. Mixtral-8x7B-32K MoE

   Mixtral-8x7B-32K一款混合专家模型（Mixtrue of Experts)，通过将多个专家模型结合在一起，
   获得更好的推理结果。据传GPT4也采用了类似的策略。目前MistralAI官方只发布了模型权重，推理代码尚未发布。
   但是大神无处不在，Github上已经有人搞出了推理代码，但是加载模型需要至少2 x 80Gb 或者 4 x 40Gb 显存。

   - 推理代码：https://github.com/dzhulgakov/llama-mistral
   - Fork 7.9k, Star 109
   - 模型权重：https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen


## 1.5 知识库

1. Langchain-Chatchat
   - https://github.com/chatchat-space/Langchain-Chatchat
   - 17.1k星

# 5. Agent