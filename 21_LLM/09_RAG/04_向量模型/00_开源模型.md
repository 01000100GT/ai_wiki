# 1. 向量模型
## 1.1 短向量模型

1. bge-base-zh-v1.5
    - 模型下载：https://modelscope.cn/models/AI-ModelScope/bge-base-zh-v1.5/summary
    - 代码：https://github.com/Alibaba-NLP/RankingGPT

## 1.2 长向量模型

1. Jina AI文本向量模型v2-base-中英双语
   - 模型下载：https://modelscope.cn/models/jinaai/jina-embeddings-v2-base-zh/summary
   - 支持中英双语的文本向量模型，它支持长达8192字符的文本编码。 该模型的研发基于BERT架构(JinaBERT)，
     JinaBERT是在BERT架构基础上的改进，首次将ALiBi应用到编码器架构中以支持更长的序列。

## 1.3 基于LLM的向量模型

1. e5-mistral-7b-instruct
   - 模型下载：https://huggingface.co/intfloat/e5-mistral-7b-instruct
   - 论文：Improving Text Embeddings with Large Language Models，https://arxiv.org/pdf/2401.00368.pdf
   - 论文：Text Embeddings by Weakly-Supervised Contrastive Pre-training
   - Github代码（非官方，26k Stars）：https://github.com/kamalkraj/e5-mistral-7b-instruct
   - （基线E5模型）官放相关代码和数据：https://github.com/microsoft/unilm/tree/master/e5

    E5-mistral-7b-instruct利用LLM产生了接近100种语言的高质量且多样化的训练数据，利用纯decoder的LLM在合成数据上进一步finetune。
    仅依靠合成数据训练得到的text embedding可以媲美目前主流的sota模型，而混合合成数据跟真实标注数据训练完成的text embedding
    模型在BEIR跟MTEB上都达到新的sota效果。

2. RankingGPT-qwen-7b
    - 模型下载：https://modelscope.cn/models/iic/RankingGPT-qwen-7b/summary
    - 代码：https://github.com/Alibaba-NLP/RankingGPT
    - text ranker based on large language models with significant in-domain and out-domain effectiveness. 
    - RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement
    - 论文：https://arxiv.org/pdf/2311.16720.pdf

# 2. ReRanker重排模型

1. bge-reranker-large
   - 模型下载：https://modelscope.cn/models/quietnight/bge-reranker-large/summary
   - 代码：https://github.com/FlagOpen/FlagEmbedding
