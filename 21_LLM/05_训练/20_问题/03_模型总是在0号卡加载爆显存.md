# 1. 问题

在LORA等微调场景，比如72B模型至少需要144GB显存，训练时，基础模型总是在0号卡加载，
导致爆显存，无法平均分配到别的卡上。

# 2. 解决方法

在使用transformer库的时候，将device_map设成auto，模型会被自动平均分配到多个卡上。

```python
 model = transformers.AutoModelForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            config=config,
            cache_dir=training_args.cache_dir,
            device_map="auto")
```

注意事项：启动的时候需要使用torchrun，而不能使用deepspeed，因为不支持auto