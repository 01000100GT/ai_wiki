# 1. 开源项目

1. Firefly mixtral 8x7b
   - https://github.com/WangRongsheng/Aurora
   - https://huggingface.co/YeungNLP/firefly-mixtral-8x7b

2. LLaMA-MoE
   - 原有模型拆解为小的专家模型
   - LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training
   - https://github.com/pjlab-sys4nlp/llama-moe

3. Deep-seek-MoE
   - 训练基于HAI-LLM框架
   - 16B MOE（2.8 B*8, 64个expert, 4个共享，63个选择top 6）
   - 设计expert和device balance loss
   - 论文地址：https://arxiv.org/abs/2401.06066
   - Github: https://github.com/deepseek-ai/DeepSeek-MoE
   - base模型：https://modelscope.cn/models/deepseek-ai/deepseek-moe-16b-base/summary
   - chat模型：https://modelscope.cn/models/deepseek-ai/deepseek-moe-16b-chat/summary


