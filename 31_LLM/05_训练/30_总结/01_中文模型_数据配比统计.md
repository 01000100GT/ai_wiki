# 1. Qwen系列

# 2. ChatGLM系列
## 2.1 ChatGLM-6B
### 2.1.1 预训练
- 1 Trillion token：主要中文和英文，另包含20国语言
- 2048 context length

## 2.2 ChatGLM2-6B
- Multi-Query-Attention推理提速42%

## 2.3 GLM4-9B
### 2.1.1 预训练
数据构成
- 10 Trillion tokens
- 8192 context length
- mostly English and Chinese
- documents from a mixture of different sources, including webpages, Wikipedia, books, code, and
papers

数据处理流程：
- deduplication：exact deduplication and fuzzy deduplication
- filtering：removing noisy documents that contain offensive language, placeholder text, source code, etc
- tokenization：converts the text into a sequence of tokens