# 1. 问题：cannot import name 'LlavaLlamaForCausalLM' from 'llava.model'

问题详情：

```
python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 187, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/usr/lib/python3.10/runpy.py", line 110, in _get_module_details
    __import__(pkg_name)
  File "/mnt/a/KI/LLaVA/llava/__init__.py", line 1, in <module>
    from .model import LlavaLlamaForCausalLM
ImportError: cannot import name 'LlavaLlamaForCausalLM' from 'llava.model' (/mnt/a/KI/LLaVA/llava/model/__init__.py)
```

解决方法：

参考Github Issue：https://github.com/haotian-liu/LLaVA/issues/1101

- flash-attn编译的和pytorch不匹配
- deepspeed版本不匹配

```bash
pip install flash-attn --no-build-isolation --no-cache-dir
```

# 2. 问题：[Usage] TypeError: LlavaLlamaForCausalLM.forward() got an unexpected keyword argument 'cache_position' #1218

问题详情：

![](.03_问题_images/问题详情.png)

解决方法：

参考Github Issue：https://github.com/haotian-liu/LLaVA/issues/1218

升级transformers到4.37.2，transformers==4.37.2