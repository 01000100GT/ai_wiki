# 1. 资源

文档：https://pytorch.org/torchtune/0.1/

# 2. 介绍

Torchtune 的诞生

随着对开放源码大型语言模型兴趣的激增，开发者们急需一种工具，能够轻松地对这些模型进行定制化的微调。现有的解决方案往往因为抽象层次过多，使得添加定制化的功能变得复杂。此外，大型模型的规模也给微调带来了不小的挑战。为了克服这些困难，torchtune 以其模块化的设计和易于扩展的特性，为开发者提供了一个全新的选择。

核心特性

- 端到端的微调支持：torchtune 提供了从数据集和模型checkpoint的下载与准备，到微调后的模型评估和本地推理的全流程支持。

- 可定制的训练构建块：支持不同模型架构和参数高效微调技术的可组合构建块，使得开发者可以根据自己的需求定制训练过程。

- 进度和性能指标的记录：通过记录训练过程中的进度和性能指标，开发者可以更好地洞察和优化训练过程。

- 模型量化：微调后的模型可以进行量化，以适应不同的部署环境。

与流行工具的集成：torchtune 与 Hugging Face Hub、PyTorch FSDP、Weights & Biases 等流行工具的集成，使得开发者能够更加便捷地使用这些资源。


设计理念

torchtune 的设计基于以下几个核心原则：

- 易于扩展：随着新技术的不断涌现，torchtune 的配方和训练循环设计为易于组合和修改，以便开发者可以快速适应新的微调用例。

- 快速上手：torchtune 旨在为不同水平的用户提供服务，无论是通过修改配置文件，还是深入代码，都能够轻松上手。

- 拥抱开源 LLM 生态：torchtune 利用开源 LLM 生态系统的丰富资源，提供与多种产品的互操作性，确保了灵活性和控制权。

# 参考

[1] 微调自己的大模型：PyTorch开源torchtune， https://mp.weixin.qq.com/s?__biz=Mzg3MDcwNjY2Mw==&mid=2247485226&idx=1&sn=06c8dbfb22795ca97add28b6de6ed938&chksm=ce88fba2f9ff72b45733d1836c01ff5d33e729183d75c7f9e21cfb4592e770d66ac8b2ca6ecb&scene=21#wechat_redirect