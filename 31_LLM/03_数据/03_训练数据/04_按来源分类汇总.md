# 1. 网络文本

## 1.1 Common Crawl

Common Crawl 数据集常被用作训练大模型的基础数据集。有时候，人们误以为它包含了互联网上的所有信息，将其誉为“涵盖整个网络数据集”。实际上，Common Crawl 并没有那么全面，但它确实覆盖了大量的公开HTML网页内容。

它遗漏了动态渲染的网站、PDF内容、任何需要登录的内容等。谷歌肯定内部拥有更全面的数据。尽管如此，它仍是一个非常大且方便的大规模网络数据来源。

就词元数量而言，原始的Common Crawl 至少有100万亿个词元。我没有计算出更精确的数字，因为对我们的目的来说这个数字相当无意义。

原始数据充满了垃圾和重复内容，因此任何用于大型语言模型（LLM）训练的实际用途都将从重度过滤开始。

## 1.2 FineWeb

包含15万亿个词元。它是自2013年以来Common Crawl转储的过滤后的英文子集。我将其视为Common Crawl所有有用英文网络文本的合理代表，因为它是最近的并且过滤得相当好。

有更大的数据集（例如 RedPajama-Data-v2 是30万亿词元），但额外的词元质量较低，因此价值有限。

# 2. 代码

## 2.1 公开可访问的代码

Stack v2 是一个涵盖大多数公开可访问代码的数据集。它在最大的变体中包含7750亿个词元，尽管如果你想要常见语言和更严格的近乎重复移除，那么大约是一半。

它基于软件遗产档案，该档案包括所有主要代码托管平台的所有公开可访问代码，不论许可证。

# 3. 书籍

## 3.1 影子图书馆

最大的影子图书馆，如Anna’s Archive，包含3100万本图书，相当于2.8万亿个词或3.9万亿个词元。我不知道这在多大程度上与谷歌图书重叠，但肯定远远少于100%。

谷歌对历史印刷图书的扫描工作是独一无二的，据我所知，从未被抓取过，而Anna’s Archive主要包含较新的电子书。

# 参考

[1] 最全盘点：人类历史上所有文本数据总量，https://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&mid=2247485448&idx=1&sn=149c4683bd8d1d2f75b444b900503823&chksm=c3546a9bf423e38dcb031eabe5d3f9002714ac13eb29d741b47d3aecde4ae3a0a88a9ce8232e&scene=21#wechat_redirect
