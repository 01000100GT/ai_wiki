# 1. 向量模型
## 1.1 短向量模型

1. bge-base-zh-v1.5
    - 模型下载：https://modelscope.cn/models/AI-ModelScope/bge-base-zh-v1.5/summary
    - 代码：https://github.com/Alibaba-NLP/RankingGPT

2. bge-large-zh-v1.5
   - 模型下载：https://modelscope.cn/models/Xorbits/bge-large-zh-v1.5/summary
   - 代码：https://github.com/Alibaba-NLP/RankingGPT

## 1.2 长向量模型

1. Jina AI文本向量模型v2-base-中英双语
   - 模型下载：https://modelscope.cn/models/jinaai/jina-embeddings-v2-base-zh/summary
   - 支持中英双语的文本向量模型，它支持长达8192字符的文本编码。 该模型的研发基于BERT架构(JinaBERT)，
     JinaBERT是在BERT架构基础上的改进，首次将ALiBi应用到编码器架构中以支持更长的序列。

2. nomic
   - Github (361 stars): https://github.com/nomic-ai/contrastors
   - 代码、数据、模型全量开放，以英文为主
   - 数据下载方法：
     - 1.注册账户：atlas.nomic.ai
     - 2.安装python客户端 (注意：nomic login会给出网址，点击后，获取api token，然后输入nomic login --token=token)
       - ```bash
         pip install nomic
         nomic login # follow prompts to login
         python -c "from nomic import atlas; print(atlas._get_datastream_credentials(name='contrastors'))"
         ```
     - 获取AWS的登录密钥（数据存储在亚马逊AWS服务上）：通过上述步骤可获取AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
     - 安装amazon aws (数据存放在该s3桶上):
       - AWS官方使用文档：https://docs.aws.amazon.com/zh_cn/cli/latest/userguide/getting-started-quickstart.html
       - windows安装方法直接安装msi包，linux安装方法如下：
         - ```bash
           curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
           unzip awscliv2.zip
           sudo ./aws/install
           ```
     - 验证是否安装成功（linux和windows都可以在命令行下操作）：
       - ```bash
         aws --version
         ```
     - 配置aws
       - aws configure：（命令行输出如下提示，可以配置，之后就可以了）
       - ```bash 
          AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
          AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
          Default region name [None]: us-west-2
          Default output format [None]: json
         ```
     - 查看数据
       - ```bash
         aws s3 ls --endpoint-url=https://9fa58365a1a3d032127970d0bd9a1290.r2.cloudflarestorage.com/ s3://contrastive
         aws s3 ls --endpoint-url=https://9fa58365a1a3d032127970d0bd9a1290.r2.cloudflarestorage.com/ s3://contrastive-index-filtered
         ```
     - 下载数据
       - 单个数据文件夹 （如果不加--recursive，会报错）
         - ```bash
           aws s3 cp --endpoint-url=https://9fa58365a1a3d032127970d0bd9a1290.r2.cloudflarestorage.com/ s3://contrastive-index-filtered/contrastive-index-filtered-000000000000.jsonl . --recursive
           ```
       - 整个s3桶下载
         - ```bash
           aws s3 sync --endpoint-url=https://9fa58365a1a3d032127970d0bd9a1290.r2.cloudflarestorage.com/ s3://contrastive . 
           ```
3. bge-m3
   - Github (4k stars): https://github.com/FlagOpen/FlagEmbedding
   - 模型下载：https://modelscope.cn/models/Xorbits/bge-m3/summary
   - 8k 多语种，支持dense/sparse/colbert等多种检索模式，可以结合3种方式，加权平均混合检索
   - 支持混合检索的向量库：目前vespa支持比较好：https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb

## 1.3 基于LLM的向量模型

1. e5-mistral-7b-instruct
   - 模型下载：https://huggingface.co/intfloat/e5-mistral-7b-instruct
   - 论文：Improving Text Embeddings with Large Language Models，https://arxiv.org/pdf/2401.00368.pdf
   - 论文：Text Embeddings by Weakly-Supervised Contrastive Pre-training
   - Github代码（非官方，26k Stars）：https://github.com/kamalkraj/e5-mistral-7b-instruct
   - （基线E5模型）官放相关代码和数据：https://github.com/microsoft/unilm/tree/master/e5

    E5-mistral-7b-instruct利用LLM产生了接近100种语言的高质量且多样化的训练数据，利用纯decoder的LLM在合成数据上进一步finetune。
    仅依靠合成数据训练得到的text embedding可以媲美目前主流的sota模型，而混合合成数据跟真实标注数据训练完成的text embedding
    模型在BEIR跟MTEB上都达到新的sota效果。

2. RankingGPT-qwen-7b
    - 模型下载：https://modelscope.cn/models/iic/RankingGPT-qwen-7b/summary
    - 代码：https://github.com/Alibaba-NLP/RankingGPT
    - text ranker based on large language models with significant in-domain and out-domain effectiveness. 
    - RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement
    - 论文：https://arxiv.org/pdf/2311.16720.pdf

    ```python
        import torch
      from modelscope import AutoTokenizer, AutoModelForCausalLM
   
      tokenizer = AutoTokenizer.from_pretrained('damo/RankingGPT-qwen-7b',trust_remote_code=True)
      model = AutoModelForCausalLM.from_pretrained('damo/RankingGPT-qwen-7b',trust_remote_code=True, device_map='cpu').eval()
   
      query='when should a baby walk'
      document='Most babies start to walk around 13 months, but your baby may start walking as early as 9 or 10 months or as late as 15 or 16 months.'
   
      context=f'Document: {document} Query:'
      example=context+query
   
      context_enc = tokenizer.encode(context, add_special_tokens=False)
      continuation_enc = tokenizer.encode(query, add_special_tokens=False)
      model_input = torch.tensor(context_enc+continuation_enc[:-1])
      continuation_len = len(continuation_enc)
      input_len, = model_input.shape
   
   
      with torch.no_grad():
          logprobs = torch.nn.functional.log_softmax(model(model_input.unsqueeze(dim=0))[0], dim=-1)[0]
   
      logprobs = logprobs[input_len-continuation_len:]
      logprobs = torch.gather(logprobs, 1, torch.tensor(continuation_enc).unsqueeze(-1)).squeeze(-1)
      score = torch.sum(logprobs)/logprobs.shape[0]
   
      print(f"Document: {document[:20] + '...'} Score: {score}") 
       ```
   
# 2. ReRanker重排模型

1. bge-reranker-large
   - 模型下载：https://modelscope.cn/models/quietnight/bge-reranker-large/summary
   - 代码：https://github.com/FlagOpen/FlagEmbedding
