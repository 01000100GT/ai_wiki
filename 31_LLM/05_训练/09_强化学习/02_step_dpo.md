# 1.DOP系列
## 1.1. MCTS-DPO
论文标题：Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning

论文地址：https://arxiv.org/abs/2405.00451

代码地址：https://github.com/YuxiXie/MCTS-DPO

# 1.2 Step-DPO
论文标题：Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs

论文地址：https://arxiv.org/abs/2406.18629

代码地址：https://github.com/dvlab-research/Step-DPO

# 参考

[1] 超越DPO！大模型精细化对齐之Step-DPO，https://mp.weixin.qq.com/s/vCs6KJ1DlfYojUJD45xRpw