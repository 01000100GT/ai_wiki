# 1. 资源下载

1. pytorch离线包：https://download.pytorch.org/whl/cu117/torch_stable.html

2. Hugging face模型下载: 
   - https://huggingface.co/THUDM/chatglm2-6b
   - 包含模型下载，模型结构代码

3. 清华大学模型下载目录：
   - https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/
   - 比国外目录下载快，且不受限制 （包含多种量化类型）

# 2. 训练推理代码

1. ChatGLM-Efficient-Tuning (**chatglm2-6b训练首推使用工具**)
   - https://github.com/hiyouga/ChatGLM-Efficient-Tuning
   - 该仓库已被存档，不在更新
   - 基于PEFT提供各类微调，非常好用（chatglm2-6b训练首推使用工具）
   - 提供RLHF
   - examples目录下有详细使用说明
   - 代码嵌套深，不易阅读
   - 问题：dp zero3训练模型保存异常 

2. chatGLM2-6B模型仓自带训练脚本

   - https://github.com/THUDM/ChatGLM2-6B
   - ptuning目录下的README.md有详细的使用说明
   - 包含数据、finetune代码（p-tuning-v2）
   - 代码简洁易读

3. ChatGLM-Finetuning
   - https://github.com/liucongg/ChatGLM-Finetuning
   - tag包0.1中包含deepspeed pipline并行样例
   
4. zero_nlp
   - 中文nlp解决方案(大模型、数据、模型、训练、推理)，包含各种微调方法以及并行训练
   - https://github.com/yuanzhoulvpi2017/zero_nlp


