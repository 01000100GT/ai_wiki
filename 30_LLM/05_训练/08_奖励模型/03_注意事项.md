# 1. 参数配置

参数库的调用：

```python
from transformers import Seq2SeqTrainingArguments, TrainingArguments
from trl import RewardConfig
```

1. TrainingArgument v.s RewardConfig

    TrainingArguments没有max_length，去掉后可替换RewardConfig，但未来TRL库仅支持RewardConfig。

2. TrainingArgument v.s Seq2SeqTrainingArguments
    
    需要删除Seq2SeqTrainingArguments中如下键值后，才能存入TrainingArguments中
    
    ```python
        del training_args_dict['sortish_sampler']
        del training_args_dict['predict_with_generate']
        del training_args_dict['generation_max_length']
        del training_args_dict['generation_num_beams']
        del training_args_dict['generation_config'] 
    ```

# 2. eval loss为nan

- 注意tokenizer的padding是left还是right
- tokenizer的padding的长度是否被设成固定长度了(比如全按32k的长度padding，显存占用会过大)
- 使用AutoModelForSequenceClassification，主模型和最后的分类层，数据类型不一致，
  一个是bf32，一个是fp16 (chatglm6b系列有这个问题，分类层写死了bf16，其它模型
  请自行注意查看)

